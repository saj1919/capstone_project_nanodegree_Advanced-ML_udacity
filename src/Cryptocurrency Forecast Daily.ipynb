{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (30.0, 15.0)\n",
    "\n",
    "import operator\n",
    "import datetime\n",
    "import re\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, CuDNNLSTM, GRU, CuDNNGRU, SimpleRNN, ConvLSTM2D, Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data from BTC_Crypto_Data.csv\n",
    "Reading and preparing data from previously generated data `BTC_Crypto_Data.csv`\n",
    "\n",
    "Filtering and taking data till `2017-12-31`\n",
    "\n",
    "Reason Being `2018` start was very-very volatile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_BTC</th>\n",
       "      <th>High_BTC</th>\n",
       "      <th>Low_BTC</th>\n",
       "      <th>Close_BTC</th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Open_LTC</th>\n",
       "      <th>High_LTC</th>\n",
       "      <th>Low_LTC</th>\n",
       "      <th>Close_LTC</th>\n",
       "      <th>Volume_LTC</th>\n",
       "      <th>...</th>\n",
       "      <th>n-transactions-excluding-chains-longer-than-100</th>\n",
       "      <th>my-wallet-n-users</th>\n",
       "      <th>n-transactions-total</th>\n",
       "      <th>transaction-fees-usd</th>\n",
       "      <th>n-transactions-per-block</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>weekday</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>318.239990</td>\n",
       "      <td>321.359985</td>\n",
       "      <td>313.540009</td>\n",
       "      <td>314.890015</td>\n",
       "      <td>4073067</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.69</td>\n",
       "      <td>623899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2726154.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>314.890015</td>\n",
       "      <td>316.399994</td>\n",
       "      <td>313.079987</td>\n",
       "      <td>315.209991</td>\n",
       "      <td>4673971</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.66</td>\n",
       "      <td>501070</td>\n",
       "      <td>...</td>\n",
       "      <td>39083.0</td>\n",
       "      <td>2730066.0</td>\n",
       "      <td>55594094.0</td>\n",
       "      <td>3391.560724</td>\n",
       "      <td>627.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>315.209991</td>\n",
       "      <td>315.829987</td>\n",
       "      <td>284.890015</td>\n",
       "      <td>287.130005</td>\n",
       "      <td>14209564</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1539198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2734236.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>287.130005</td>\n",
       "      <td>289.940002</td>\n",
       "      <td>255.869995</td>\n",
       "      <td>264.720001</td>\n",
       "      <td>24255392</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1748672</td>\n",
       "      <td>...</td>\n",
       "      <td>48874.0</td>\n",
       "      <td>2738654.0</td>\n",
       "      <td>55762496.0</td>\n",
       "      <td>5496.656838</td>\n",
       "      <td>692.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>264.720001</td>\n",
       "      <td>278.320007</td>\n",
       "      <td>262.890015</td>\n",
       "      <td>274.839996</td>\n",
       "      <td>14995382</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.94</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2273109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2743273.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>274.839996</td>\n",
       "      <td>285.109985</td>\n",
       "      <td>271.519989</td>\n",
       "      <td>282.269989</td>\n",
       "      <td>6097640</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1791736</td>\n",
       "      <td>...</td>\n",
       "      <td>47820.0</td>\n",
       "      <td>2747920.0</td>\n",
       "      <td>55943409.0</td>\n",
       "      <td>3558.725583</td>\n",
       "      <td>639.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>282.269989</td>\n",
       "      <td>296.059998</td>\n",
       "      <td>280.739990</td>\n",
       "      <td>291.339996</td>\n",
       "      <td>8827164</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2504600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2752885.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>291.339996</td>\n",
       "      <td>292.709991</td>\n",
       "      <td>278.989990</td>\n",
       "      <td>282.690002</td>\n",
       "      <td>5796312</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.12</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1977972</td>\n",
       "      <td>...</td>\n",
       "      <td>51405.0</td>\n",
       "      <td>2757765.0</td>\n",
       "      <td>56135447.0</td>\n",
       "      <td>4000.835905</td>\n",
       "      <td>943.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-09</th>\n",
       "      <td>282.690002</td>\n",
       "      <td>290.589996</td>\n",
       "      <td>277.279999</td>\n",
       "      <td>287.970001</td>\n",
       "      <td>5963203</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1519433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2763084.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-10</th>\n",
       "      <td>287.970001</td>\n",
       "      <td>289.250000</td>\n",
       "      <td>271.980011</td>\n",
       "      <td>273.350006</td>\n",
       "      <td>4147357</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.63</td>\n",
       "      <td>982939</td>\n",
       "      <td>...</td>\n",
       "      <td>43989.0</td>\n",
       "      <td>2768262.0</td>\n",
       "      <td>56346553.0</td>\n",
       "      <td>4292.492121</td>\n",
       "      <td>833.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-11</th>\n",
       "      <td>273.350006</td>\n",
       "      <td>278.010010</td>\n",
       "      <td>263.760010</td>\n",
       "      <td>264.769989</td>\n",
       "      <td>4122751</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.66</td>\n",
       "      <td>989462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2773180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-12</th>\n",
       "      <td>264.769989</td>\n",
       "      <td>273.079987</td>\n",
       "      <td>263.299988</td>\n",
       "      <td>269.329987</td>\n",
       "      <td>6899315</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.73</td>\n",
       "      <td>915666</td>\n",
       "      <td>...</td>\n",
       "      <td>45494.0</td>\n",
       "      <td>2779043.0</td>\n",
       "      <td>56538695.0</td>\n",
       "      <td>8812.784783</td>\n",
       "      <td>655.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-13</th>\n",
       "      <td>269.329987</td>\n",
       "      <td>269.100006</td>\n",
       "      <td>217.100006</td>\n",
       "      <td>221.289993</td>\n",
       "      <td>22745960</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2214844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2783890.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-14</th>\n",
       "      <td>221.289993</td>\n",
       "      <td>230.860001</td>\n",
       "      <td>157.289993</td>\n",
       "      <td>164.919998</td>\n",
       "      <td>32911586</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1658128</td>\n",
       "      <td>...</td>\n",
       "      <td>53403.0</td>\n",
       "      <td>2788850.0</td>\n",
       "      <td>56742266.0</td>\n",
       "      <td>2802.028495</td>\n",
       "      <td>607.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open_BTC    High_BTC     Low_BTC   Close_BTC  Volume_BTC  \\\n",
       "Date                                                                     \n",
       "2015-01-01  318.239990  321.359985  313.540009  314.890015     4073067   \n",
       "2015-01-02  314.890015  316.399994  313.079987  315.209991     4673971   \n",
       "2015-01-03  315.209991  315.829987  284.890015  287.130005    14209564   \n",
       "2015-01-04  287.130005  289.940002  255.869995  264.720001    24255392   \n",
       "2015-01-05  264.720001  278.320007  262.890015  274.839996    14995382   \n",
       "2015-01-06  274.839996  285.109985  271.519989  282.269989     6097640   \n",
       "2015-01-07  282.269989  296.059998  280.739990  291.339996     8827164   \n",
       "2015-01-08  291.339996  292.709991  278.989990  282.690002     5796312   \n",
       "2015-01-09  282.690002  290.589996  277.279999  287.970001     5963203   \n",
       "2015-01-10  287.970001  289.250000  271.980011  273.350006     4147357   \n",
       "2015-01-11  273.350006  278.010010  263.760010  264.769989     4122751   \n",
       "2015-01-12  264.769989  273.079987  263.299988  269.329987     6899315   \n",
       "2015-01-13  269.329987  269.100006  217.100006  221.289993    22745960   \n",
       "2015-01-14  221.289993  230.860001  157.289993  164.919998    32911586   \n",
       "\n",
       "            Open_LTC  High_LTC  Low_LTC  Close_LTC  Volume_LTC  ...   \\\n",
       "Date                                                            ...    \n",
       "2015-01-01      2.71      2.72     2.67       2.69      623899  ...    \n",
       "2015-01-02      2.69      2.69     2.64       2.66      501070  ...    \n",
       "2015-01-03      2.66      2.66     2.17       2.18     1539198  ...    \n",
       "2015-01-04      2.18      2.20     1.89       1.95     1748672  ...    \n",
       "2015-01-05      1.95      2.17     1.94       2.11     2273109  ...    \n",
       "2015-01-06      2.11      2.13     2.03       2.10     1791736  ...    \n",
       "2015-01-07      2.10      2.16     2.06       2.11     2504600  ...    \n",
       "2015-01-08      2.11      2.12     1.99       2.00     1977972  ...    \n",
       "2015-01-09      2.00      2.01     1.94       1.98     1519433  ...    \n",
       "2015-01-10      1.98      1.99     1.55       1.63      982939  ...    \n",
       "2015-01-11      1.63      1.88     1.62       1.66      989462  ...    \n",
       "2015-01-12      1.66      1.79     1.66       1.73      915666  ...    \n",
       "2015-01-13      1.73      1.73     1.53       1.54     2214844  ...    \n",
       "2015-01-14      1.54      1.57     1.12       1.12     1658128  ...    \n",
       "\n",
       "            n-transactions-excluding-chains-longer-than-100  \\\n",
       "Date                                                          \n",
       "2015-01-01                                              0.0   \n",
       "2015-01-02                                          39083.0   \n",
       "2015-01-03                                              0.0   \n",
       "2015-01-04                                          48874.0   \n",
       "2015-01-05                                              0.0   \n",
       "2015-01-06                                          47820.0   \n",
       "2015-01-07                                              0.0   \n",
       "2015-01-08                                          51405.0   \n",
       "2015-01-09                                              0.0   \n",
       "2015-01-10                                          43989.0   \n",
       "2015-01-11                                              0.0   \n",
       "2015-01-12                                          45494.0   \n",
       "2015-01-13                                              0.0   \n",
       "2015-01-14                                          53403.0   \n",
       "\n",
       "            my-wallet-n-users  n-transactions-total  transaction-fees-usd  \\\n",
       "Date                                                                        \n",
       "2015-01-01          2726154.0                   0.0              0.000000   \n",
       "2015-01-02          2730066.0            55594094.0           3391.560724   \n",
       "2015-01-03          2734236.0                   0.0              0.000000   \n",
       "2015-01-04          2738654.0            55762496.0           5496.656838   \n",
       "2015-01-05          2743273.0                   0.0              0.000000   \n",
       "2015-01-06          2747920.0            55943409.0           3558.725583   \n",
       "2015-01-07          2752885.0                   0.0              0.000000   \n",
       "2015-01-08          2757765.0            56135447.0           4000.835905   \n",
       "2015-01-09          2763084.0                   0.0              0.000000   \n",
       "2015-01-10          2768262.0            56346553.0           4292.492121   \n",
       "2015-01-11          2773180.0                   0.0              0.000000   \n",
       "2015-01-12          2779043.0            56538695.0           8812.784783   \n",
       "2015-01-13          2783890.0                   0.0              0.000000   \n",
       "2015-01-14          2788850.0            56742266.0           2802.028495   \n",
       "\n",
       "            n-transactions-per-block  day_of_month  month  year  weekday  week  \n",
       "Date                                                                            \n",
       "2015-01-01                       0.0             1      1  2015        3     1  \n",
       "2015-01-02                     627.0             2      1  2015        4     1  \n",
       "2015-01-03                       0.0             3      1  2015        5     1  \n",
       "2015-01-04                     692.0             4      1  2015        6     1  \n",
       "2015-01-05                       0.0             5      1  2015        0     2  \n",
       "2015-01-06                     639.0             6      1  2015        1     2  \n",
       "2015-01-07                       0.0             7      1  2015        2     2  \n",
       "2015-01-08                     943.0             8      1  2015        3     2  \n",
       "2015-01-09                       0.0             9      1  2015        4     2  \n",
       "2015-01-10                     833.0            10      1  2015        5     2  \n",
       "2015-01-11                       0.0            11      1  2015        6     2  \n",
       "2015-01-12                     655.0            12      1  2015        0     3  \n",
       "2015-01-13                       0.0            13      1  2015        1     3  \n",
       "2015-01-14                     607.0            14      1  2015        2     3  \n",
       "\n",
       "[14 rows x 107 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../output/BTC_Crypto_Data.csv', sep='\\t', header=0)\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df = df[df['Date']<'2018-01-01']\n",
    "df['day_of_month'] = df['Date'].dt.day\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['year'] = df['Date'].dt.year\n",
    "df['weekday'] = df['Date'].dt.weekday\n",
    "df['week'] = df['Date'].dt.week\n",
    "df.set_index('Date', inplace=True)\n",
    "df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_BTC</th>\n",
       "      <th>High_BTC</th>\n",
       "      <th>Low_BTC</th>\n",
       "      <th>Close_BTC</th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Open_LTC</th>\n",
       "      <th>High_LTC</th>\n",
       "      <th>Low_LTC</th>\n",
       "      <th>Close_LTC</th>\n",
       "      <th>Volume_LTC</th>\n",
       "      <th>...</th>\n",
       "      <th>n-transactions-excluding-chains-longer-than-100</th>\n",
       "      <th>my-wallet-n-users</th>\n",
       "      <th>n-transactions-total</th>\n",
       "      <th>transaction-fees-usd</th>\n",
       "      <th>n-transactions-per-block</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>weekday</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-27</th>\n",
       "      <td>15757.019531</td>\n",
       "      <td>16514.589844</td>\n",
       "      <td>14534.660156</td>\n",
       "      <td>15416.639648</td>\n",
       "      <td>2162831128</td>\n",
       "      <td>279.470001</td>\n",
       "      <td>285.320007</td>\n",
       "      <td>252.869995</td>\n",
       "      <td>264.070007</td>\n",
       "      <td>219103646</td>\n",
       "      <td>...</td>\n",
       "      <td>168263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286521958.0</td>\n",
       "      <td>1.223477e+07</td>\n",
       "      <td>1972.064103</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <td>15416.339844</td>\n",
       "      <td>15505.509766</td>\n",
       "      <td>13466.070313</td>\n",
       "      <td>14398.700195</td>\n",
       "      <td>2425912717</td>\n",
       "      <td>264.070007</td>\n",
       "      <td>265.649994</td>\n",
       "      <td>224.070007</td>\n",
       "      <td>249.860001</td>\n",
       "      <td>281385850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <td>14398.450195</td>\n",
       "      <td>15109.809570</td>\n",
       "      <td>13951.080078</td>\n",
       "      <td>14392.570313</td>\n",
       "      <td>1733583750</td>\n",
       "      <td>249.839996</td>\n",
       "      <td>258.359985</td>\n",
       "      <td>236.389999</td>\n",
       "      <td>243.130005</td>\n",
       "      <td>207115905</td>\n",
       "      <td>...</td>\n",
       "      <td>182523.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287180835.0</td>\n",
       "      <td>1.150415e+07</td>\n",
       "      <td>2144.387879</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>2017</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-30</th>\n",
       "      <td>14392.139648</td>\n",
       "      <td>14461.459961</td>\n",
       "      <td>11962.089844</td>\n",
       "      <td>12531.519531</td>\n",
       "      <td>2387311023</td>\n",
       "      <td>243.130005</td>\n",
       "      <td>243.419998</td>\n",
       "      <td>202.020004</td>\n",
       "      <td>212.070007</td>\n",
       "      <td>268737310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>12532.379883</td>\n",
       "      <td>14241.820313</td>\n",
       "      <td>12359.429688</td>\n",
       "      <td>13850.400391</td>\n",
       "      <td>1492142483</td>\n",
       "      <td>212.070007</td>\n",
       "      <td>235.399994</td>\n",
       "      <td>207.949997</td>\n",
       "      <td>226.520004</td>\n",
       "      <td>187138183</td>\n",
       "      <td>...</td>\n",
       "      <td>204038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287815664.0</td>\n",
       "      <td>7.722286e+06</td>\n",
       "      <td>1781.730061</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open_BTC      High_BTC       Low_BTC     Close_BTC  \\\n",
       "Date                                                                 \n",
       "2017-12-27  15757.019531  16514.589844  14534.660156  15416.639648   \n",
       "2017-12-28  15416.339844  15505.509766  13466.070313  14398.700195   \n",
       "2017-12-29  14398.450195  15109.809570  13951.080078  14392.570313   \n",
       "2017-12-30  14392.139648  14461.459961  11962.089844  12531.519531   \n",
       "2017-12-31  12532.379883  14241.820313  12359.429688  13850.400391   \n",
       "\n",
       "            Volume_BTC    Open_LTC    High_LTC     Low_LTC   Close_LTC  \\\n",
       "Date                                                                     \n",
       "2017-12-27  2162831128  279.470001  285.320007  252.869995  264.070007   \n",
       "2017-12-28  2425912717  264.070007  265.649994  224.070007  249.860001   \n",
       "2017-12-29  1733583750  249.839996  258.359985  236.389999  243.130005   \n",
       "2017-12-30  2387311023  243.130005  243.419998  202.020004  212.070007   \n",
       "2017-12-31  1492142483  212.070007  235.399994  207.949997  226.520004   \n",
       "\n",
       "            Volume_LTC  ...   n-transactions-excluding-chains-longer-than-100  \\\n",
       "Date                    ...                                                     \n",
       "2017-12-27   219103646  ...                                          168263.0   \n",
       "2017-12-28   281385850  ...                                               0.0   \n",
       "2017-12-29   207115905  ...                                          182523.0   \n",
       "2017-12-30   268737310  ...                                               0.0   \n",
       "2017-12-31   187138183  ...                                          204038.0   \n",
       "\n",
       "            my-wallet-n-users  n-transactions-total  transaction-fees-usd  \\\n",
       "Date                                                                        \n",
       "2017-12-27                0.0           286521958.0          1.223477e+07   \n",
       "2017-12-28                0.0                   0.0          0.000000e+00   \n",
       "2017-12-29                0.0           287180835.0          1.150415e+07   \n",
       "2017-12-30                0.0                   0.0          0.000000e+00   \n",
       "2017-12-31                0.0           287815664.0          7.722286e+06   \n",
       "\n",
       "            n-transactions-per-block  day_of_month  month  year  weekday  week  \n",
       "Date                                                                            \n",
       "2017-12-27               1972.064103            27     12  2017        2    52  \n",
       "2017-12-28                  0.000000            28     12  2017        3    52  \n",
       "2017-12-29               2144.387879            29     12  2017        4    52  \n",
       "2017-12-30                  0.000000            30     12  2017        5    52  \n",
       "2017-12-31               1781.730061            31     12  2017        6    52  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert series to supervised learning\n",
    "\n",
    "This function is used to provide more features/references to RNN network for previous days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    temp_df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(temp_df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        \n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(temp_df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    \n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1314, 1605)\n"
     ]
    }
   ],
   "source": [
    "# ensure all data is float\n",
    "values = df.values.astype('float32')\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(values, 14, 1)\n",
    "\n",
    "print reframed.shape\n",
    "for col in reframed.columns:\n",
    "    if \"(t)\" in col:\n",
    "        if col != 'var4(t)':\n",
    "            del reframed[col]\n",
    "\n",
    "reframed.fillna(0, inplace=True)\n",
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "values = scaler.fit_transform(reframed.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(1250, 1, 1498) (1250,) (64, 1, 1498) (64,) (90, 1, 1498) (90,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train = values[0:1250, :]\n",
    "test = values[1250:, :]\n",
    "valid = values[np.random.choice(values[600:,:].shape[0], 90, replace=False), :]\n",
    "\n",
    "print type(values)\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "valid_X, valid_y = valid[:, :-1], valid[:, -1]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "valid_X = valid_X.reshape((valid_X.shape[0], 1, valid_X.shape[1]))\n",
    "print train_X.shape, train_y.shape, test_X.shape, test_y.shape, valid_X.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 256)               1797120   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,798,401\n",
      "Trainable params: 1,797,889\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(256, input_shape=(train_X.shape[1], train_X.shape[2]))) #, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(GRU(384))\n",
    "# model.add(PReLU())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1250 samples, validate on 64 samples\n",
      "Epoch 1/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 3.4429\n",
      "Epoch 00001: val_loss improved from inf to 23.33527, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 3.3722 - val_loss: 23.3353\n",
      "Epoch 2/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 1.6879\n",
      "Epoch 00002: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 91us/step - loss: 2.7941 - val_loss: 55.4435\n",
      "Epoch 3/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 8.1018\n",
      "Epoch 00003: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 92us/step - loss: 6.0410 - val_loss: 54.3171\n",
      "Epoch 4/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 4.8436\n",
      "Epoch 00004: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 89us/step - loss: 3.6046 - val_loss: 48.5334\n",
      "Epoch 5/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 1.3686\n",
      "Epoch 00005: val_loss improved from 23.33527 to 6.22790, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 154us/step - loss: 1.3561 - val_loss: 6.2279\n",
      "Epoch 6/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.8098\n",
      "Epoch 00006: val_loss improved from 6.22790 to 1.86935, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 159us/step - loss: 0.8694 - val_loss: 1.8694\n",
      "Epoch 7/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.6979\n",
      "Epoch 00007: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 88us/step - loss: 0.6998 - val_loss: 10.2847\n",
      "Epoch 8/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.6931\n",
      "Epoch 00008: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 87us/step - loss: 0.6690 - val_loss: 11.3018\n",
      "Epoch 9/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.6315\n",
      "Epoch 00009: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 87us/step - loss: 0.6034 - val_loss: 3.6757\n",
      "Epoch 10/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.6440\n",
      "Epoch 00010: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 85us/step - loss: 0.6390 - val_loss: 2.8089\n",
      "Epoch 11/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.5562\n",
      "Epoch 00011: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 84us/step - loss: 0.5589 - val_loss: 5.4409\n",
      "Epoch 12/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.5000\n",
      "Epoch 00012: val_loss improved from 1.86935 to 0.83209, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 153us/step - loss: 0.4931 - val_loss: 0.8321\n",
      "Epoch 13/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.5089\n",
      "Epoch 00013: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 88us/step - loss: 0.4941 - val_loss: 1.7800\n",
      "Epoch 14/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.4483\n",
      "Epoch 00014: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 85us/step - loss: 0.4403 - val_loss: 4.4541\n",
      "Epoch 15/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.4539\n",
      "Epoch 00015: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 85us/step - loss: 0.4421 - val_loss: 2.5723\n",
      "Epoch 16/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.4286\n",
      "Epoch 00016: val_loss improved from 0.83209 to 0.45002, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 151us/step - loss: 0.4000 - val_loss: 0.4500\n",
      "Epoch 17/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.3960\n",
      "Epoch 00017: val_loss improved from 0.45002 to 0.43110, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 152us/step - loss: 0.3786 - val_loss: 0.4311\n",
      "Epoch 18/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.3316\n",
      "Epoch 00018: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 86us/step - loss: 0.3214 - val_loss: 1.4080\n",
      "Epoch 19/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.3298\n",
      "Epoch 00019: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 86us/step - loss: 0.3188 - val_loss: 1.5950\n",
      "Epoch 20/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.2843\n",
      "Epoch 00020: val_loss improved from 0.43110 to 0.38875, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 150us/step - loss: 0.2739 - val_loss: 0.3887\n",
      "Epoch 21/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.2926\n",
      "Epoch 00021: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 84us/step - loss: 0.2618 - val_loss: 0.7970\n",
      "Epoch 22/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.2283\n",
      "Epoch 00022: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.2111 - val_loss: 1.4128\n",
      "Epoch 23/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.1940\n",
      "Epoch 00023: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 85us/step - loss: 0.1834 - val_loss: 0.6377\n",
      "Epoch 24/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.1814\n",
      "Epoch 00024: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 83us/step - loss: 0.1703 - val_loss: 0.4889\n",
      "Epoch 25/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.1696\n",
      "Epoch 00025: val_loss improved from 0.38875 to 0.27236, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 148us/step - loss: 0.1586 - val_loss: 0.2724\n",
      "Epoch 26/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.1483\n",
      "Epoch 00026: val_loss improved from 0.27236 to 0.17944, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 146us/step - loss: 0.1373 - val_loss: 0.1794\n",
      "Epoch 27/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.1428\n",
      "Epoch 00027: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 84us/step - loss: 0.1324 - val_loss: 0.2427\n",
      "Epoch 28/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00028: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.1039 - val_loss: 0.3678\n",
      "Epoch 29/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0997\n",
      "Epoch 00029: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0894 - val_loss: 0.3109\n",
      "Epoch 30/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0982\n",
      "Epoch 00030: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0895 - val_loss: 0.2937\n",
      "Epoch 31/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0845\n",
      "Epoch 00031: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0792 - val_loss: 0.2616\n",
      "Epoch 32/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0745\n",
      "Epoch 00032: val_loss improved from 0.17944 to 0.13244, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 145us/step - loss: 0.0679 - val_loss: 0.1324\n",
      "Epoch 33/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0681\n",
      "Epoch 00033: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0629 - val_loss: 0.1666\n",
      "Epoch 34/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0561\n",
      "Epoch 00034: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 83us/step - loss: 0.0534 - val_loss: 0.2175\n",
      "Epoch 35/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0616\n",
      "Epoch 00035: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0548 - val_loss: 0.2066\n",
      "Epoch 36/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.0518\n",
      "Epoch 00036: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0471 - val_loss: 0.1955\n",
      "Epoch 37/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0438\n",
      "Epoch 00037: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0403 - val_loss: 0.1737\n",
      "Epoch 38/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0412\n",
      "Epoch 00038: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0384 - val_loss: 0.1702\n",
      "Epoch 39/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0361\n",
      "Epoch 00039: val_loss improved from 0.13244 to 0.11073, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 145us/step - loss: 0.0335 - val_loss: 0.1107\n",
      "Epoch 40/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.0361\n",
      "Epoch 00040: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 83us/step - loss: 0.0321 - val_loss: 0.1287\n",
      "Epoch 41/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0359\n",
      "Epoch 00041: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0338 - val_loss: 0.1424\n",
      "Epoch 42/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0304\n",
      "Epoch 00042: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0289 - val_loss: 0.1549\n",
      "Epoch 43/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0304\n",
      "Epoch 00043: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0289 - val_loss: 0.1422\n",
      "Epoch 44/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0331\n",
      "Epoch 00044: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0304 - val_loss: 0.1528\n",
      "Epoch 45/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0339\n",
      "Epoch 00045: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0304 - val_loss: 0.1701\n",
      "Epoch 46/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0258\n",
      "Epoch 00046: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0240 - val_loss: 0.2039\n",
      "Epoch 47/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0266\n",
      "Epoch 00047: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0242 - val_loss: 0.2050\n",
      "Epoch 48/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0232\n",
      "Epoch 00048: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0214 - val_loss: 0.2117\n",
      "Epoch 49/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0225\n",
      "Epoch 00049: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0209 - val_loss: 0.2252\n",
      "Epoch 50/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0248\n",
      "Epoch 00050: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0222 - val_loss: 0.2403\n",
      "Epoch 51/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0231\n",
      "Epoch 00051: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0212 - val_loss: 0.2206\n",
      "Epoch 52/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0192\n",
      "Epoch 00052: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0184 - val_loss: 0.2124\n",
      "Epoch 53/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0187\n",
      "Epoch 00053: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0178 - val_loss: 0.2240\n",
      "Epoch 54/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0172\n",
      "Epoch 00054: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0161 - val_loss: 0.2193\n",
      "Epoch 55/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0187\n",
      "Epoch 00055: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0170 - val_loss: 0.2294\n",
      "Epoch 56/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0176\n",
      "Epoch 00056: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0169 - val_loss: 0.2389\n",
      "Epoch 57/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0149\n",
      "Epoch 00057: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0148 - val_loss: 0.2251\n",
      "Epoch 58/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0157\n",
      "Epoch 00058: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0146 - val_loss: 0.2218\n",
      "Epoch 59/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0161\n",
      "Epoch 00059: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0154 - val_loss: 0.2243\n",
      "Epoch 60/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0162\n",
      "Epoch 00060: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0157 - val_loss: 0.2215\n",
      "Epoch 61/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0155\n",
      "Epoch 00061: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0147 - val_loss: 0.2344\n",
      "Epoch 62/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0152\n",
      "Epoch 00062: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0146 - val_loss: 0.2334\n",
      "Epoch 63/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0134\n",
      "Epoch 00063: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0129 - val_loss: 0.2352\n",
      "Epoch 64/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0142\n",
      "Epoch 00064: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0137 - val_loss: 0.2235\n",
      "Epoch 65/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0157\n",
      "Epoch 00065: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0148 - val_loss: 0.2183\n",
      "Epoch 66/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0131\n",
      "Epoch 00066: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0125 - val_loss: 0.2199\n",
      "Epoch 67/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0125\n",
      "Epoch 00067: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0123 - val_loss: 0.2197\n",
      "Epoch 68/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0123\n",
      "Epoch 00068: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0123 - val_loss: 0.2070\n",
      "Epoch 69/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0121\n",
      "Epoch 00069: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0121 - val_loss: 0.1946\n",
      "Epoch 70/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0107\n",
      "Epoch 00070: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0108 - val_loss: 0.1829\n",
      "Epoch 71/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0134\n",
      "Epoch 00071: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0126 - val_loss: 0.1856\n",
      "Epoch 72/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0114\n",
      "Epoch 00072: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0111 - val_loss: 0.1874\n",
      "Epoch 73/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0111\n",
      "Epoch 00073: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0111 - val_loss: 0.1864\n",
      "Epoch 74/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0114\n",
      "Epoch 00074: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0114 - val_loss: 0.1830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0106\n",
      "Epoch 00075: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0106 - val_loss: 0.1858\n",
      "Epoch 76/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0110\n",
      "Epoch 00076: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0108 - val_loss: 0.1779\n",
      "Epoch 77/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0102\n",
      "Epoch 00077: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0105 - val_loss: 0.1819\n",
      "Epoch 78/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0121\n",
      "Epoch 00078: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0118 - val_loss: 0.1964\n",
      "Epoch 79/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0104\n",
      "Epoch 00079: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0104 - val_loss: 0.2047\n",
      "Epoch 80/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0096\n",
      "Epoch 00080: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0099 - val_loss: 0.2001\n",
      "Epoch 81/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0095\n",
      "Epoch 00081: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0097 - val_loss: 0.1923\n",
      "Epoch 82/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0099\n",
      "Epoch 00082: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0101 - val_loss: 0.1854\n",
      "Epoch 83/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0094\n",
      "Epoch 00083: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0101 - val_loss: 0.1797\n",
      "Epoch 84/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0092\n",
      "Epoch 00084: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0096 - val_loss: 0.1802\n",
      "Epoch 85/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0092\n",
      "Epoch 00085: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0095 - val_loss: 0.1821\n",
      "Epoch 86/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0084\n",
      "Epoch 00086: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0090 - val_loss: 0.1952\n",
      "Epoch 87/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0072\n",
      "Epoch 00087: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0081 - val_loss: 0.2030\n",
      "Epoch 88/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0078\n",
      "Epoch 00088: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0083 - val_loss: 0.1985\n",
      "Epoch 89/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0080\n",
      "Epoch 00089: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0087 - val_loss: 0.1837\n",
      "Epoch 90/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0091\n",
      "Epoch 00090: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0096 - val_loss: 0.1656\n",
      "Epoch 91/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0072\n",
      "Epoch 00091: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0080 - val_loss: 0.1664\n",
      "Epoch 92/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0095\n",
      "Epoch 00092: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0098 - val_loss: 0.1714\n",
      "Epoch 93/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0086\n",
      "Epoch 00093: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0091 - val_loss: 0.1751\n",
      "Epoch 94/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0106\n",
      "Epoch 00094: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0105 - val_loss: 0.1731\n",
      "Epoch 95/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0081\n",
      "Epoch 00095: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0087 - val_loss: 0.1946\n",
      "Epoch 96/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.0074\n",
      "Epoch 00096: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0078 - val_loss: 0.1964\n",
      "Epoch 97/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0078\n",
      "Epoch 00097: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0083 - val_loss: 0.1905\n",
      "Epoch 98/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0069\n",
      "Epoch 00098: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0076 - val_loss: 0.1919\n",
      "Epoch 99/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0075\n",
      "Epoch 00099: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0082 - val_loss: 0.1816\n",
      "Epoch 100/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0069\n",
      "Epoch 00100: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0076 - val_loss: 0.1879\n",
      "Epoch 101/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0071\n",
      "Epoch 00101: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0079 - val_loss: 0.1888\n",
      "Epoch 102/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0070\n",
      "Epoch 00102: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0080 - val_loss: 0.1959\n",
      "Epoch 103/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0075\n",
      "Epoch 00103: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0084 - val_loss: 0.2010\n",
      "Epoch 104/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0061\n",
      "Epoch 00104: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0072 - val_loss: 0.1988\n",
      "Epoch 105/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0067\n",
      "Epoch 00105: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0076 - val_loss: 0.1980\n",
      "Epoch 106/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0058\n",
      "Epoch 00106: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0070 - val_loss: 0.1951\n",
      "Epoch 107/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0065\n",
      "Epoch 00107: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0074 - val_loss: 0.1971\n",
      "Epoch 108/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0055\n",
      "Epoch 00108: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0066 - val_loss: 0.1926\n",
      "Epoch 109/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0080\n",
      "Epoch 00109: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0086 - val_loss: 0.1884\n",
      "Epoch 110/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0057\n",
      "Epoch 00110: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0069 - val_loss: 0.1897\n",
      "Epoch 111/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0064\n",
      "Epoch 00111: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0073 - val_loss: 0.1960\n",
      "Epoch 112/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0060\n",
      "Epoch 00112: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0070 - val_loss: 0.1938\n",
      "Epoch 113/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0073\n",
      "Epoch 00113: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0080 - val_loss: 0.1847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0063\n",
      "Epoch 00114: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0072 - val_loss: 0.2016\n",
      "Epoch 115/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0061\n",
      "Epoch 00115: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0071 - val_loss: 0.2012\n",
      "Epoch 116/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0054\n",
      "Epoch 00116: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0065 - val_loss: 0.2018\n",
      "Epoch 117/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0052\n",
      "Epoch 00117: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0063 - val_loss: 0.2081\n",
      "Epoch 118/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0053\n",
      "Epoch 00118: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0065 - val_loss: 0.2012\n",
      "Epoch 119/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0060\n",
      "Epoch 00119: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 83us/step - loss: 0.0071 - val_loss: 0.1884\n",
      "Epoch 120/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0059\n",
      "Epoch 00120: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0070 - val_loss: 0.1751\n",
      "Epoch 121/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0053\n",
      "Epoch 00121: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0065 - val_loss: 0.1780\n",
      "Epoch 122/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.0057\n",
      "Epoch 00122: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 84us/step - loss: 0.0062 - val_loss: 0.1799\n",
      "Epoch 123/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0051\n",
      "Epoch 00123: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0064 - val_loss: 0.1897\n",
      "Epoch 124/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0071\n",
      "Epoch 00124: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0078 - val_loss: 0.1881\n",
      "Epoch 125/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0049\n",
      "Epoch 00125: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0060 - val_loss: 0.1879\n",
      "Epoch 126/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0056\n",
      "Epoch 00126: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0066 - val_loss: 0.1907\n",
      "Epoch 127/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0050\n",
      "Epoch 00127: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0064 - val_loss: 0.1931\n",
      "Epoch 128/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0055\n",
      "Epoch 00128: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0066 - val_loss: 0.1957\n",
      "Epoch 129/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0049\n",
      "Epoch 00129: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 83us/step - loss: 0.0062 - val_loss: 0.1981\n",
      "Epoch 130/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0049\n",
      "Epoch 00130: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0060 - val_loss: 0.1883\n",
      "Epoch 131/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0049\n",
      "Epoch 00131: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0062 - val_loss: 0.1867\n",
      "Epoch 132/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0045\n",
      "Epoch 00132: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0058 - val_loss: 0.1977\n",
      "Epoch 133/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0043\n",
      "Epoch 00133: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0058 - val_loss: 0.2073\n",
      "Epoch 134/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0046\n",
      "Epoch 00134: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0060 - val_loss: 0.2115\n",
      "Epoch 135/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0050\n",
      "Epoch 00135: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0063 - val_loss: 0.2056\n",
      "Epoch 136/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0048\n",
      "Epoch 00136: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0061 - val_loss: 0.2065\n",
      "Epoch 137/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0044\n",
      "Epoch 00137: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0058 - val_loss: 0.2079\n",
      "Epoch 138/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0050\n",
      "Epoch 00138: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0062 - val_loss: 0.2009\n",
      "Epoch 139/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00139: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0056 - val_loss: 0.1943\n",
      "Epoch 140/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00140: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0056 - val_loss: 0.1810\n",
      "Epoch 141/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0038\n",
      "Epoch 00141: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0053 - val_loss: 0.1867\n",
      "Epoch 142/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0047\n",
      "Epoch 00142: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0061 - val_loss: 0.1985\n",
      "Epoch 143/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0046\n",
      "Epoch 00143: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0059 - val_loss: 0.2091\n",
      "Epoch 144/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0045\n",
      "Epoch 00144: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0059 - val_loss: 0.2026\n",
      "Epoch 145/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00145: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0056 - val_loss: 0.1990\n",
      "Epoch 146/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0045\n",
      "Epoch 00146: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0057 - val_loss: 0.1989\n",
      "Epoch 147/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00147: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0056 - val_loss: 0.1943\n",
      "Epoch 148/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0044\n",
      "Epoch 00148: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0058 - val_loss: 0.1876\n",
      "Epoch 149/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0045\n",
      "Epoch 00149: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0058 - val_loss: 0.1879\n",
      "Epoch 150/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0047\n",
      "Epoch 00150: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0060 - val_loss: 0.1900\n",
      "Epoch 151/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0056\n",
      "Epoch 00151: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0067 - val_loss: 0.1912\n",
      "Epoch 152/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0055\n",
      "Epoch 00152: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0066 - val_loss: 0.1847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0039\n",
      "Epoch 00153: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0055 - val_loss: 0.1729\n",
      "Epoch 154/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00154: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0056 - val_loss: 0.1718\n",
      "Epoch 155/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0038\n",
      "Epoch 00155: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0051 - val_loss: 0.1697\n",
      "Epoch 156/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0044\n",
      "Epoch 00156: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0057 - val_loss: 0.1691\n",
      "Epoch 157/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0041\n",
      "Epoch 00157: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0056 - val_loss: 0.1660\n",
      "Epoch 158/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0040\n",
      "Epoch 00158: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0056 - val_loss: 0.1741\n",
      "Epoch 159/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0044\n",
      "Epoch 00159: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0058 - val_loss: 0.1797\n",
      "Epoch 160/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0038\n",
      "Epoch 00160: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0053 - val_loss: 0.1788\n",
      "Epoch 161/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0043\n",
      "Epoch 00161: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0057 - val_loss: 0.1806\n",
      "Epoch 162/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0038\n",
      "Epoch 00162: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0054 - val_loss: 0.1869\n",
      "Epoch 163/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0035\n",
      "Epoch 00163: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0051 - val_loss: 0.1822\n",
      "Epoch 164/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0047\n",
      "Epoch 00164: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0059 - val_loss: 0.1677\n",
      "Epoch 165/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0037\n",
      "Epoch 00165: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0052 - val_loss: 0.1619\n",
      "Epoch 166/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0033\n",
      "Epoch 00166: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0050 - val_loss: 0.1553\n",
      "Epoch 167/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0036\n",
      "Epoch 00167: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0052 - val_loss: 0.1569\n",
      "Epoch 168/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0043\n",
      "Epoch 00168: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0057 - val_loss: 0.1626\n",
      "Epoch 169/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0044\n",
      "Epoch 00169: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0058 - val_loss: 0.1636\n",
      "Epoch 170/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0052\n",
      "Epoch 00170: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0064 - val_loss: 0.1651\n",
      "Epoch 171/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0046\n",
      "Epoch 00171: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0059 - val_loss: 0.1670\n",
      "Epoch 172/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0039\n",
      "Epoch 00172: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0055 - val_loss: 0.1645\n",
      "Epoch 173/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0036\n",
      "Epoch 00173: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0052 - val_loss: 0.1611\n",
      "Epoch 174/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00174: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0057 - val_loss: 0.1566\n",
      "Epoch 175/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0038\n",
      "Epoch 00175: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0054 - val_loss: 0.1524\n",
      "Epoch 176/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0037\n",
      "Epoch 00176: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0052 - val_loss: 0.1458\n",
      "Epoch 177/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0029\n",
      "Epoch 00177: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0047 - val_loss: 0.1431\n",
      "Epoch 178/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0032\n",
      "Epoch 00178: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0049 - val_loss: 0.1474\n",
      "Epoch 179/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0030\n",
      "Epoch 00179: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0048 - val_loss: 0.1498\n",
      "Epoch 180/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00180: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0055 - val_loss: 0.1516\n",
      "Epoch 181/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0038\n",
      "Epoch 00181: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0053 - val_loss: 0.1520\n",
      "Epoch 182/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0035\n",
      "Epoch 00182: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0052 - val_loss: 0.1538\n",
      "Epoch 183/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0029\n",
      "Epoch 00183: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0045 - val_loss: 0.1569\n",
      "Epoch 184/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0040\n",
      "Epoch 00184: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0054 - val_loss: 0.1550\n",
      "Epoch 185/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0032\n",
      "Epoch 00185: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0049 - val_loss: 0.1451\n",
      "Epoch 186/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00186: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.1418\n",
      "Epoch 187/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0032\n",
      "Epoch 00187: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0049 - val_loss: 0.1446\n",
      "Epoch 188/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0031\n",
      "Epoch 00188: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0049 - val_loss: 0.1509\n",
      "Epoch 189/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0029\n",
      "Epoch 00189: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0047 - val_loss: 0.1558\n",
      "Epoch 190/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.0037\n",
      "Epoch 00190: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0048 - val_loss: 0.1488\n",
      "Epoch 191/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0031\n",
      "Epoch 00191: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0047 - val_loss: 0.1431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0030\n",
      "Epoch 00192: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0047 - val_loss: 0.1403\n",
      "Epoch 193/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0034\n",
      "Epoch 00193: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 85us/step - loss: 0.0050 - val_loss: 0.1351\n",
      "Epoch 194/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0035\n",
      "Epoch 00194: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0052 - val_loss: 0.1327\n",
      "Epoch 195/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00195: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0045 - val_loss: 0.1321\n",
      "Epoch 196/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0031\n",
      "Epoch 00196: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0047 - val_loss: 0.1310\n",
      "Epoch 197/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00197: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 78us/step - loss: 0.0044 - val_loss: 0.1317\n",
      "Epoch 198/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0035\n",
      "Epoch 00198: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0051 - val_loss: 0.1342\n",
      "Epoch 199/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00199: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0044 - val_loss: 0.1286\n",
      "Epoch 200/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0029\n",
      "Epoch 00200: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0047 - val_loss: 0.1244\n",
      "Epoch 201/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0035\n",
      "Epoch 00201: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0052 - val_loss: 0.1271\n",
      "Epoch 202/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0034\n",
      "Epoch 00202: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0050 - val_loss: 0.1324\n",
      "Epoch 203/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00203: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0044 - val_loss: 0.1281\n",
      "Epoch 204/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.0025\n",
      "Epoch 00204: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0043 - val_loss: 0.1243\n",
      "Epoch 205/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0032\n",
      "Epoch 00205: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0049 - val_loss: 0.1189\n",
      "Epoch 206/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0035\n",
      "Epoch 00206: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0051 - val_loss: 0.1162\n",
      "Epoch 207/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0030\n",
      "Epoch 00207: val_loss improved from 0.11073 to 0.11024, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 146us/step - loss: 0.0047 - val_loss: 0.1102\n",
      "Epoch 208/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0041\n",
      "Epoch 00208: val_loss improved from 0.11024 to 0.10180, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 145us/step - loss: 0.0055 - val_loss: 0.1018\n",
      "Epoch 209/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00209: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0042 - val_loss: 0.1023\n",
      "Epoch 210/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00210: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0043 - val_loss: 0.1041\n",
      "Epoch 211/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0045\n",
      "Epoch 00211: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0058 - val_loss: 0.1150\n",
      "Epoch 212/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0033\n",
      "Epoch 00212: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 78us/step - loss: 0.0049 - val_loss: 0.1215\n",
      "Epoch 213/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0031\n",
      "Epoch 00213: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0048 - val_loss: 0.1197\n",
      "Epoch 214/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00214: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0047 - val_loss: 0.1116\n",
      "Epoch 215/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00215: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0047 - val_loss: 0.1019\n",
      "Epoch 216/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00216: val_loss improved from 0.10180 to 0.10033, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 150us/step - loss: 0.0046 - val_loss: 0.1003\n",
      "Epoch 217/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00217: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0045 - val_loss: 0.1016\n",
      "Epoch 218/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0031\n",
      "Epoch 00218: val_loss improved from 0.10033 to 0.09962, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 142us/step - loss: 0.0048 - val_loss: 0.0996\n",
      "Epoch 219/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0030\n",
      "Epoch 00219: val_loss improved from 0.09962 to 0.09518, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 143us/step - loss: 0.0047 - val_loss: 0.0952\n",
      "Epoch 220/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0027\n",
      "Epoch 00220: val_loss improved from 0.09518 to 0.08918, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 141us/step - loss: 0.0046 - val_loss: 0.0892\n",
      "Epoch 221/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00221: val_loss improved from 0.08918 to 0.08650, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 144us/step - loss: 0.0043 - val_loss: 0.0865\n",
      "Epoch 222/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00222: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0044 - val_loss: 0.0879\n",
      "Epoch 223/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00223: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0929\n",
      "Epoch 224/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0029\n",
      "Epoch 00224: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0047 - val_loss: 0.1014\n",
      "Epoch 225/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00225: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 78us/step - loss: 0.0043 - val_loss: 0.0950\n",
      "Epoch 226/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.0027\n",
      "Epoch 00226: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0045 - val_loss: 0.0908\n",
      "Epoch 227/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00227: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0882\n",
      "Epoch 228/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00228: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0043 - val_loss: 0.0917\n",
      "Epoch 229/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00229: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 78us/step - loss: 0.0043 - val_loss: 0.0942\n",
      "Epoch 230/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00230: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0044 - val_loss: 0.0937\n",
      "Epoch 231/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00231: val_loss improved from 0.08650 to 0.08430, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 146us/step - loss: 0.0043 - val_loss: 0.0843\n",
      "Epoch 232/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00232: val_loss improved from 0.08430 to 0.07943, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 155us/step - loss: 0.0043 - val_loss: 0.0794\n",
      "Epoch 233/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0029\n",
      "Epoch 00233: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0046 - val_loss: 0.0828\n",
      "Epoch 234/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00234: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0044 - val_loss: 0.0851\n",
      "Epoch 235/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00235: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0041 - val_loss: 0.0860\n",
      "Epoch 236/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0039\n",
      "Epoch 00236: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0054 - val_loss: 0.0869\n",
      "Epoch 237/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0044\n",
      "Epoch 00237: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0058 - val_loss: 0.0882\n",
      "Epoch 238/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0031\n",
      "Epoch 00238: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0047 - val_loss: 0.0858\n",
      "Epoch 239/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0029\n",
      "Epoch 00239: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0047 - val_loss: 0.0854\n",
      "Epoch 240/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0027\n",
      "Epoch 00240: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0045 - val_loss: 0.0822\n",
      "Epoch 241/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0030\n",
      "Epoch 00241: val_loss improved from 0.07943 to 0.07681, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 149us/step - loss: 0.0047 - val_loss: 0.0768\n",
      "Epoch 242/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0029\n",
      "Epoch 00242: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0046 - val_loss: 0.0786\n",
      "Epoch 243/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00243: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0040 - val_loss: 0.0800\n",
      "Epoch 244/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0027\n",
      "Epoch 00244: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0045 - val_loss: 0.0819\n",
      "Epoch 245/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00245: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0043 - val_loss: 0.0886\n",
      "Epoch 246/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00246: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0041 - val_loss: 0.0960\n",
      "Epoch 247/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00247: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0041 - val_loss: 0.0981\n",
      "Epoch 248/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00248: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0046 - val_loss: 0.0934\n",
      "Epoch 249/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00249: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0841\n",
      "Epoch 250/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00250: val_loss improved from 0.07681 to 0.07481, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 162us/step - loss: 0.0039 - val_loss: 0.0748\n",
      "Epoch 251/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00251: val_loss improved from 0.07481 to 0.06919, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 149us/step - loss: 0.0042 - val_loss: 0.0692\n",
      "Epoch 252/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00252: val_loss improved from 0.06919 to 0.06757, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 230us/step - loss: 0.0043 - val_loss: 0.0676\n",
      "Epoch 253/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00253: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0043 - val_loss: 0.0713\n",
      "Epoch 254/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00254: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0044 - val_loss: 0.0734\n",
      "Epoch 255/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00255: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0045 - val_loss: 0.0823\n",
      "Epoch 256/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00256: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0043 - val_loss: 0.0906\n",
      "Epoch 257/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00257: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 83us/step - loss: 0.0042 - val_loss: 0.0899\n",
      "Epoch 258/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00258: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0044 - val_loss: 0.0820\n",
      "Epoch 259/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00259: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0704\n",
      "Epoch 260/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00260: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0041 - val_loss: 0.0694\n",
      "Epoch 261/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00261: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0041 - val_loss: 0.0751\n",
      "Epoch 262/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00262: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0040 - val_loss: 0.0763\n",
      "Epoch 263/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00263: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0041 - val_loss: 0.0741\n",
      "Epoch 264/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00264: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00265: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0040 - val_loss: 0.0683\n",
      "Epoch 266/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00266: val_loss improved from 0.06757 to 0.06722, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 146us/step - loss: 0.0041 - val_loss: 0.0672\n",
      "Epoch 267/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0029\n",
      "Epoch 00267: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0047 - val_loss: 0.0681\n",
      "Epoch 268/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0031\n",
      "Epoch 00268: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0048 - val_loss: 0.0703\n",
      "Epoch 269/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00269: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0046 - val_loss: 0.0720\n",
      "Epoch 270/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00270: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0705\n",
      "Epoch 271/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00271: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0042 - val_loss: 0.0675\n",
      "Epoch 272/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00272: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0041 - val_loss: 0.0731\n",
      "Epoch 273/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00273: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0041 - val_loss: 0.0762\n",
      "Epoch 274/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00274: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0042 - val_loss: 0.0691\n",
      "Epoch 275/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00275: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0041 - val_loss: 0.0700\n",
      "Epoch 276/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00276: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0039 - val_loss: 0.0691\n",
      "Epoch 277/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00277: val_loss improved from 0.06722 to 0.06386, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 148us/step - loss: 0.0040 - val_loss: 0.0639\n",
      "Epoch 278/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00278: val_loss improved from 0.06386 to 0.06183, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 153us/step - loss: 0.0043 - val_loss: 0.0618\n",
      "Epoch 279/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0027\n",
      "Epoch 00279: val_loss improved from 0.06183 to 0.06084, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 147us/step - loss: 0.0045 - val_loss: 0.0608\n",
      "Epoch 280/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00280: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0043 - val_loss: 0.0620\n",
      "Epoch 281/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00281: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0040 - val_loss: 0.0632\n",
      "Epoch 282/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00282: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0040 - val_loss: 0.0622\n",
      "Epoch 283/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00283: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0041 - val_loss: 0.0641\n",
      "Epoch 284/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00284: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0042 - val_loss: 0.0670\n",
      "Epoch 285/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00285: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0042 - val_loss: 0.0690\n",
      "Epoch 286/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00286: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0042 - val_loss: 0.0687\n",
      "Epoch 287/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00287: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0720\n",
      "Epoch 288/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00288: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0043 - val_loss: 0.0668\n",
      "Epoch 289/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00289: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0045 - val_loss: 0.0666\n",
      "Epoch 290/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00290: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0041 - val_loss: 0.0662\n",
      "Epoch 291/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00291: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0042 - val_loss: 0.0654\n",
      "Epoch 292/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00292: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0646\n",
      "Epoch 293/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00293: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0040 - val_loss: 0.0645\n",
      "Epoch 294/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00294: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0039 - val_loss: 0.0700\n",
      "Epoch 295/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00295: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0044 - val_loss: 0.0674\n",
      "Epoch 296/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00296: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0632\n",
      "Epoch 297/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00297: val_loss improved from 0.06084 to 0.05599, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 144us/step - loss: 0.0041 - val_loss: 0.0560\n",
      "Epoch 298/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00298: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0594\n",
      "Epoch 299/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00299: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0044 - val_loss: 0.0622\n",
      "Epoch 300/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00300: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 84us/step - loss: 0.0039 - val_loss: 0.0671\n",
      "Epoch 301/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00301: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0040 - val_loss: 0.0654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 302/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00302: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0041 - val_loss: 0.0671\n",
      "Epoch 303/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00303: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0701\n",
      "Epoch 304/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00304: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0042 - val_loss: 0.0686\n",
      "Epoch 305/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00305: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0041 - val_loss: 0.0657\n",
      "Epoch 306/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00306: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0039 - val_loss: 0.0620\n",
      "Epoch 307/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00307: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 78us/step - loss: 0.0041 - val_loss: 0.0615\n",
      "Epoch 308/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00308: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0041 - val_loss: 0.0582\n",
      "Epoch 309/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00309: val_loss improved from 0.05599 to 0.05516, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 143us/step - loss: 0.0042 - val_loss: 0.0552\n",
      "Epoch 310/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00310: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0042 - val_loss: 0.0567\n",
      "Epoch 311/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00311: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0041 - val_loss: 0.0579\n",
      "Epoch 312/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00312: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 86us/step - loss: 0.0040 - val_loss: 0.0582\n",
      "Epoch 313/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00313: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0554\n",
      "Epoch 314/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00314: val_loss improved from 0.05516 to 0.05489, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 151us/step - loss: 0.0044 - val_loss: 0.0549\n",
      "Epoch 315/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00315: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0038 - val_loss: 0.0613\n",
      "Epoch 316/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00316: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0040 - val_loss: 0.0655\n",
      "Epoch 317/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00317: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0040 - val_loss: 0.0623\n",
      "Epoch 318/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00318: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0606\n",
      "Epoch 319/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00319: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0039 - val_loss: 0.0648\n",
      "Epoch 320/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00320: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0041 - val_loss: 0.0630\n",
      "Epoch 321/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00321: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0638\n",
      "Epoch 322/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00322: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0630\n",
      "Epoch 323/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00323: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0042 - val_loss: 0.0660\n",
      "Epoch 324/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00324: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0039 - val_loss: 0.0673\n",
      "Epoch 325/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00325: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0040 - val_loss: 0.0733\n",
      "Epoch 326/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00326: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0038 - val_loss: 0.0793\n",
      "Epoch 327/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00327: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0038 - val_loss: 0.0793\n",
      "Epoch 328/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00328: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 78us/step - loss: 0.0040 - val_loss: 0.0780\n",
      "Epoch 329/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00329: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0041 - val_loss: 0.0684\n",
      "Epoch 330/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00330: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0044 - val_loss: 0.0679\n",
      "Epoch 331/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00331: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0042 - val_loss: 0.0627\n",
      "Epoch 332/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00332: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0662\n",
      "Epoch 333/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00333: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 78us/step - loss: 0.0039 - val_loss: 0.0640\n",
      "Epoch 334/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00334: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0038 - val_loss: 0.0608\n",
      "Epoch 335/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00335: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0040 - val_loss: 0.0654\n",
      "Epoch 336/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00336: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0041 - val_loss: 0.0705\n",
      "Epoch 337/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00337: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0043 - val_loss: 0.0720\n",
      "Epoch 338/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00338: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0038 - val_loss: 0.0660\n",
      "Epoch 339/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00339: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00340: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0040 - val_loss: 0.0644\n",
      "Epoch 341/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00341: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0604\n",
      "Epoch 342/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00342: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0038 - val_loss: 0.0596\n",
      "Epoch 343/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00343: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0040 - val_loss: 0.0602\n",
      "Epoch 344/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00344: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0041 - val_loss: 0.0595\n",
      "Epoch 345/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00345: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0618\n",
      "Epoch 346/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00346: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0040 - val_loss: 0.0601\n",
      "Epoch 347/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00347: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0042 - val_loss: 0.0577\n",
      "Epoch 348/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0027\n",
      "Epoch 00348: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 83us/step - loss: 0.0045 - val_loss: 0.0550\n",
      "Epoch 349/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0036\n",
      "Epoch 00349: val_loss improved from 0.05489 to 0.05440, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 146us/step - loss: 0.0051 - val_loss: 0.0544\n",
      "Epoch 350/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0038\n",
      "Epoch 00350: val_loss improved from 0.05440 to 0.05370, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 145us/step - loss: 0.0053 - val_loss: 0.0537\n",
      "Epoch 351/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00351: val_loss improved from 0.05370 to 0.04982, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 149us/step - loss: 0.0043 - val_loss: 0.0498\n",
      "Epoch 352/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00352: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 83us/step - loss: 0.0039 - val_loss: 0.0518\n",
      "Epoch 353/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00353: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0042 - val_loss: 0.0526\n",
      "Epoch 354/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00354: val_loss improved from 0.04982 to 0.04824, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 139us/step - loss: 0.0044 - val_loss: 0.0482\n",
      "Epoch 355/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00355: val_loss improved from 0.04824 to 0.04389, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 144us/step - loss: 0.0039 - val_loss: 0.0439\n",
      "Epoch 356/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00356: val_loss improved from 0.04389 to 0.04134, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 146us/step - loss: 0.0038 - val_loss: 0.0413\n",
      "Epoch 357/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00357: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0428\n",
      "Epoch 358/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0029\n",
      "Epoch 00358: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0046 - val_loss: 0.0482\n",
      "Epoch 359/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00359: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0046 - val_loss: 0.0544\n",
      "Epoch 360/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00360: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 83us/step - loss: 0.0039 - val_loss: 0.0509\n",
      "Epoch 361/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00361: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0039 - val_loss: 0.0575\n",
      "Epoch 362/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00362: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0590\n",
      "Epoch 363/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00363: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0040 - val_loss: 0.0567\n",
      "Epoch 364/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00364: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0040 - val_loss: 0.0547\n",
      "Epoch 365/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00365: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0037 - val_loss: 0.0547\n",
      "Epoch 366/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00366: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0554\n",
      "Epoch 367/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00367: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0552\n",
      "Epoch 368/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00368: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0435\n",
      "Epoch 369/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00369: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0421\n",
      "Epoch 370/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00370: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0042 - val_loss: 0.0415\n",
      "Epoch 371/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0031\n",
      "Epoch 00371: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0048 - val_loss: 0.0460\n",
      "Epoch 372/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0033\n",
      "Epoch 00372: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0050 - val_loss: 0.0452\n",
      "Epoch 373/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00373: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0041 - val_loss: 0.0493\n",
      "Epoch 374/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00374: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0452\n",
      "Epoch 375/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00375: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0039 - val_loss: 0.0452\n",
      "Epoch 376/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00376: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0042 - val_loss: 0.0460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 377/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00377: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0037 - val_loss: 0.0443\n",
      "Epoch 378/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00378: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0041 - val_loss: 0.0482\n",
      "Epoch 379/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00379: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 78us/step - loss: 0.0039 - val_loss: 0.0581\n",
      "Epoch 380/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00380: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0040 - val_loss: 0.0551\n",
      "Epoch 381/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00381: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0036 - val_loss: 0.0523\n",
      "Epoch 382/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00382: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0036 - val_loss: 0.0503\n",
      "Epoch 383/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00383: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0045 - val_loss: 0.0466\n",
      "Epoch 384/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00384: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0462\n",
      "Epoch 385/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00385: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0468\n",
      "Epoch 386/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00386: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0042 - val_loss: 0.0503\n",
      "Epoch 387/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00387: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0037 - val_loss: 0.0505\n",
      "Epoch 388/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00388: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0040 - val_loss: 0.0530\n",
      "Epoch 389/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00389: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0037 - val_loss: 0.0567\n",
      "Epoch 390/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00390: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0040 - val_loss: 0.0557\n",
      "Epoch 391/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00391: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0039 - val_loss: 0.0526\n",
      "Epoch 392/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00392: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0038 - val_loss: 0.0503\n",
      "Epoch 393/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00393: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0037 - val_loss: 0.0496\n",
      "Epoch 394/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00394: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0040 - val_loss: 0.0512\n",
      "Epoch 395/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00395: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0043 - val_loss: 0.0496\n",
      "Epoch 396/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00396: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0536\n",
      "Epoch 397/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00397: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0039 - val_loss: 0.0596\n",
      "Epoch 398/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00398: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0658\n",
      "Epoch 399/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00399: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0036 - val_loss: 0.0629\n",
      "Epoch 400/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00400: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0038 - val_loss: 0.0603\n",
      "Epoch 401/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00401: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0570\n",
      "Epoch 402/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00402: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0604\n",
      "Epoch 403/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00403: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0817\n",
      "Epoch 404/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00404: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0037 - val_loss: 0.0514\n",
      "Epoch 405/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00405: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0036 - val_loss: 0.0470\n",
      "Epoch 406/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00406: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0035 - val_loss: 0.0428\n",
      "Epoch 407/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00407: val_loss improved from 0.04134 to 0.04023, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 150us/step - loss: 0.0036 - val_loss: 0.0402\n",
      "Epoch 408/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00408: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0037 - val_loss: 0.0539\n",
      "Epoch 409/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00409: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0526\n",
      "Epoch 410/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00410: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0039 - val_loss: 0.0468\n",
      "Epoch 411/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0027\n",
      "Epoch 00411: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0045 - val_loss: 0.0450\n",
      "Epoch 412/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00412: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0041 - val_loss: 0.0440\n",
      "Epoch 413/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00413: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0037 - val_loss: 0.0452\n",
      "Epoch 414/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00414: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0466\n",
      "Epoch 415/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00415: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0039 - val_loss: 0.0451\n",
      "Epoch 416/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00416: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0470\n",
      "Epoch 417/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00417: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0037 - val_loss: 0.0530\n",
      "Epoch 418/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00418: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0037 - val_loss: 0.0560\n",
      "Epoch 419/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00419: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0038 - val_loss: 0.0543\n",
      "Epoch 420/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00420: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0037 - val_loss: 0.0446\n",
      "Epoch 421/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00421: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0445\n",
      "Epoch 422/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00422: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0499\n",
      "Epoch 423/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00423: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0036 - val_loss: 0.0571\n",
      "Epoch 424/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00424: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0606\n",
      "Epoch 425/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00425: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0656\n",
      "Epoch 426/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00426: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0761\n",
      "Epoch 427/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00427: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0038 - val_loss: 0.0730\n",
      "Epoch 428/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00428: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0040 - val_loss: 0.0648\n",
      "Epoch 429/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00429: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0625\n",
      "Epoch 430/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00430: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0041 - val_loss: 0.0639\n",
      "Epoch 431/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00431: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0038 - val_loss: 0.0559\n",
      "Epoch 432/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00432: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0040 - val_loss: 0.0515\n",
      "Epoch 433/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00433: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0531\n",
      "Epoch 434/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00434: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0504\n",
      "Epoch 435/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00435: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0039 - val_loss: 0.0502\n",
      "Epoch 436/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00436: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 83us/step - loss: 0.0040 - val_loss: 0.0509\n",
      "Epoch 437/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00437: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0039 - val_loss: 0.0695\n",
      "Epoch 438/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00438: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0649\n",
      "Epoch 439/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00439: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0549\n",
      "Epoch 440/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00440: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0587\n",
      "Epoch 441/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00441: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0038 - val_loss: 0.0471\n",
      "Epoch 442/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00442: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0042 - val_loss: 0.0430\n",
      "Epoch 443/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00443: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0043 - val_loss: 0.0450\n",
      "Epoch 444/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00444: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0043 - val_loss: 0.0439\n",
      "Epoch 445/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0035\n",
      "Epoch 00445: val_loss improved from 0.04023 to 0.03763, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 147us/step - loss: 0.0050 - val_loss: 0.0376\n",
      "Epoch 446/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0044\n",
      "Epoch 00446: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0057 - val_loss: 0.0466\n",
      "Epoch 447/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0031\n",
      "Epoch 00447: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0047 - val_loss: 0.0553\n",
      "Epoch 448/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00448: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0041 - val_loss: 0.0595\n",
      "Epoch 449/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00449: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0036 - val_loss: 0.0653\n",
      "Epoch 450/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00450: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0037 - val_loss: 0.0639\n",
      "Epoch 451/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00451: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0036 - val_loss: 0.0542\n",
      "Epoch 452/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00452: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0499\n",
      "Epoch 453/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00453: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0035 - val_loss: 0.0514\n",
      "Epoch 454/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00454: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0037 - val_loss: 0.0594\n",
      "Epoch 455/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00455: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0641\n",
      "Epoch 456/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00456: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0036 - val_loss: 0.0621\n",
      "Epoch 457/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00457: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 78us/step - loss: 0.0035 - val_loss: 0.0583\n",
      "Epoch 458/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00458: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0566\n",
      "Epoch 459/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00459: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0036 - val_loss: 0.0603\n",
      "Epoch 460/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00460: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0036 - val_loss: 0.0579\n",
      "Epoch 461/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00461: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0035 - val_loss: 0.0571\n",
      "Epoch 462/500\n",
      " 768/1250 [=================>............] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00462: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0037 - val_loss: 0.0620\n",
      "Epoch 463/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00463: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0798\n",
      "Epoch 464/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00464: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0039 - val_loss: 0.0681\n",
      "Epoch 465/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00465: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0041 - val_loss: 0.0523\n",
      "Epoch 466/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00466: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0044 - val_loss: 0.0441\n",
      "Epoch 467/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0026\n",
      "Epoch 00467: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 78us/step - loss: 0.0043 - val_loss: 0.0387\n",
      "Epoch 468/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0031\n",
      "Epoch 00468: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0047 - val_loss: 0.0398\n",
      "Epoch 469/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.0026\n",
      "Epoch 00469: val_loss improved from 0.03763 to 0.03751, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 151us/step - loss: 0.0043 - val_loss: 0.0375\n",
      "Epoch 470/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00470: val_loss improved from 0.03751 to 0.03571, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 148us/step - loss: 0.0040 - val_loss: 0.0357\n",
      "Epoch 471/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00471: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 85us/step - loss: 0.0035 - val_loss: 0.0373\n",
      "Epoch 472/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00472: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0036 - val_loss: 0.0386\n",
      "Epoch 473/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00473: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0395\n",
      "Epoch 474/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00474: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0036 - val_loss: 0.0439\n",
      "Epoch 475/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00475: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0036 - val_loss: 0.0449\n",
      "Epoch 476/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00476: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0038 - val_loss: 0.0477\n",
      "Epoch 477/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00477: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0404\n",
      "Epoch 478/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00478: val_loss improved from 0.03571 to 0.03511, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 145us/step - loss: 0.0041 - val_loss: 0.0351\n",
      "Epoch 479/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00479: val_loss improved from 0.03511 to 0.03098, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 152us/step - loss: 0.0039 - val_loss: 0.0310\n",
      "Epoch 480/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0023\n",
      "Epoch 00480: val_loss improved from 0.03098 to 0.03049, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 145us/step - loss: 0.0042 - val_loss: 0.0305\n",
      "Epoch 481/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00481: val_loss improved from 0.03049 to 0.02896, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 149us/step - loss: 0.0040 - val_loss: 0.0290\n",
      "Epoch 482/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00482: val_loss improved from 0.02896 to 0.02739, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 144us/step - loss: 0.0035 - val_loss: 0.0274\n",
      "Epoch 483/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00483: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0039 - val_loss: 0.0323\n",
      "Epoch 484/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00484: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0042 - val_loss: 0.0327\n",
      "Epoch 485/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00485: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0039 - val_loss: 0.0338\n",
      "Epoch 486/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00486: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0035 - val_loss: 0.0330\n",
      "Epoch 487/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00487: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0036 - val_loss: 0.0306\n",
      "Epoch 488/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00488: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0038 - val_loss: 0.0353\n",
      "Epoch 489/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00489: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0037 - val_loss: 0.0321\n",
      "Epoch 490/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00490: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0282\n",
      "Epoch 491/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0027\n",
      "Epoch 00491: val_loss improved from 0.02739 to 0.02645, saving model to ../output/daily_out/model_lstm.hdf5\n",
      "1250/1250 [==============================] - 0s 143us/step - loss: 0.0044 - val_loss: 0.0264\n",
      "Epoch 492/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00492: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 81us/step - loss: 0.0046 - val_loss: 0.0292\n",
      "Epoch 493/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      "Epoch 00493: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0044 - val_loss: 0.0321\n",
      "Epoch 494/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00494: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0040 - val_loss: 0.0350\n",
      "Epoch 495/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00495: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 78us/step - loss: 0.0036 - val_loss: 0.0369\n",
      "Epoch 496/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00496: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0035 - val_loss: 0.0350\n",
      "Epoch 497/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00497: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0036 - val_loss: 0.0351\n",
      "Epoch 498/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00498: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0271\n",
      "Epoch 499/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00499: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 82us/step - loss: 0.0036 - val_loss: 0.0283\n",
      "Epoch 500/500\n",
      " 896/1250 [====================>.........] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00500: val_loss did not improve\n",
      "1250/1250 [==============================] - 0s 80us/step - loss: 0.0037 - val_loss: 0.0295\n"
     ]
    }
   ],
   "source": [
    "# Learn and Track the model training\n",
    "earlystop = EarlyStopping(monitor='loss', min_delta=0.0000001, patience=64, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath='../output/daily_out/model_lstm.hdf5', verbose=1, save_best_only=True)\n",
    "callbacks_list = [earlystop, checkpointer]\n",
    "\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=500, batch_size=128, \n",
    "                    validation_data=(test_X, test_y), callbacks=callbacks_list, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot history of loss and val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABqkAAANSCAYAAAD23SQrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ac8fa5a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(history.history['loss'][10:], label='train')\n",
    "plt.plot(history.history['val_loss'][10:], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best Model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(256, input_shape=(train_X.shape[1], train_X.shape[2])))#, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(GRU(384))\n",
    "# model.add(PReLU())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.load_weights(\"../output/daily_out/model_lstm.hdf5\")\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast :  [0.32565149664878845, 0.3352092206478119, 0.31853151321411133, 0.3301503360271454, 0.32450661063194275, 0.33723184466362, 0.33596891164779663, 0.33845120668411255, 0.3334689140319824, 0.32901322841644287, 0.32318422198295593, 0.334396094083786, 0.3347611725330353, 0.3475506007671356, 0.32391318678855896, 0.33061569929122925, 0.3362747132778168, 0.3383707106113434, 0.3443610668182373, 0.33400681614875793, 0.34881678223609924, 0.35283613204956055, 0.35781431198120117, 0.3703461289405823, 0.37630417943000793, 0.3869926333427429, 0.3841734230518341, 0.38345009088516235, 0.36479079723358154, 0.37527066469192505, 0.38435447216033936, 0.40977370738983154, 0.42343828082084656, 0.4376769959926605, 0.4673970639705658, 0.48872676491737366, 0.49115467071533203, 0.528424859046936, 0.5372323393821716, 0.5727401375770569, 0.5737687349319458, 0.6278731226921082, 0.6119980216026306, 0.6480709910392761, 0.6120099425315857, 0.6305145025253296, 0.6041884422302246, 0.6621525287628174, 0.6511313319206238, 0.7189640998840332, 0.7637454867362976, 0.8098054528236389, 0.8517910838127136, 0.8626039624214172, 0.9008459448814392, 0.911409318447113, 0.9448527693748474, 0.9437693953514099, 1.0042222738265991, 1.0125977993011475, 1.0661710500717163, 1.0515912771224976, 1.0893933773040771, 1.0632482767105103]\n",
      "Test Y :  [0.31190938 0.31069773 0.32774416 0.34268323 0.35764784 0.3642853\n",
      " 0.37532145 0.37666395 0.35422876 0.36171132 0.37952155 0.36311066\n",
      " 0.33371684 0.32193723 0.29786444 0.33145678 0.33534664 0.37112033\n",
      " 0.4008619  0.39284703 0.39706796 0.41071352 0.42124763 0.4137025\n",
      " 0.42071894 0.40918958 0.41895935 0.44783705 0.47722772 0.4988527\n",
      " 0.50786394 0.50430936 0.50998694 0.5576763  0.56034887 0.5777352\n",
      " 0.597427   0.5996803  0.7082506  0.8699111  0.8280614  0.76527965\n",
      " 0.7765504  0.8637674  0.88208956 0.8405329  0.8499742  0.9092498\n",
      " 1.         0.9854134  0.9805444  0.9050189  0.84966457 0.8063994\n",
      " 0.70383984 0.7419769  0.7103559  0.71262586 0.8128872  0.79516506\n",
      " 0.7420937  0.7417741  0.6447462  0.7135075 ]\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "print \"Forecast : \", yhat[:,0].tolist()\n",
    "print \"Test Y : \", test_y\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "\n",
    "# invert scaling for forecast\n",
    "inv_yhat = np.concatenate((test_X, yhat), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,-1]\n",
    "\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_X, test_y), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 3119.414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5ac3a3dc10>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABsIAAANSCAYAAADf2tusAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIkSVKDZBAmSZIkSZIkSZKkBun/BzUaMpHv92wgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ac3db03d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print 'Test RMSE: %.3f' % rmse\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result['actual'] = inv_y\n",
    "result['prediction'] = inv_yhat\n",
    "result.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
